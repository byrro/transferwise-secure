#!/usr/bin/python3 Python3
from calendar import timegm
from collections import namedtuple
from datetime import datetime, timedelta
import json
import logging
import os
import queue
from typing import Callable, List, Tuple, NamedTuple

import boto3
import botocore

from retry_queue import RetryLimitQueue


log = logging.getLogger(os.environ.get('LOGGER_NAME'))

LOCAL_ENV = os.environ.get('AWS_SAM_LOCAL') == 'true'

# General constants
LOCAL_PORT = 8000
CONNECTION_TIMEOUT = 3
READ_TIMEOUT = 3

# Batch processing constants
BATCH_GET_MAX_SIZE = int(os.environ.get('DYNAMODB_BATCH_GET_MAX_SIZE', 100))
BATCH_GET_MAX_RETRIES = int(os.environ.get('DYNAMODB_BATCH_GET_MAX_RETRIES', 3))  # NOQA
BATCH_WRITE_MAX_SIZE = int(os.environ.get('DYNAMODB_BATCH_WRITE_MAX_SIZE', 25))
BATCH_WRITE_MAX_RETRIES = int(os.environ.get('DYNAMODB_BATCH_WRITE_MAX_RETRIES', 3))  # NOQA

# Namedtuples for block structure responses
table_operations = namedtuple('operations', 'batch_get batch_put')
manager = namedtuple('batch_manager', [
    # Attributes
    'queue',
    'status',
    'responses',

    # Methods
    'next_batch',
    'parse_response',
])


class BatchQueueStatus():
    FULL = 'FULL'
    EMPTY = 'EMPTY'


def boto3_local_client(
        endpoint_url: str ='http://host.docker.internal',
        local_port: int = LOCAL_PORT,
        verify: bool = False,
        region_name: str = 'us-east-1',
        aws_access_key_id: str = 'LOCAL_ACCESS_KEY_ID',
        aws_secret_access_key: str = 'LOCAL_SECRET_ACCESS_KEY',
        connect_timeout: int = CONNECTION_TIMEOUT,
        read_timeout: int = READ_TIMEOUT,
        ) -> botocore.client.BaseClient:
    return boto3.client('dynamodb', **client_kwargs)


def local_env_client_args(client_kwargs):
    import yaml

    with open('dynamodb-local.yaml', 'r') as file:
        ddb = yaml.load(file.read(), Loader=yaml.SafeLoader)
        ddb_port = ddb['services']['dynamodb-local']['ports'][0].split(':')[0]

    return {
        **client_kwargs,
        'endpoint_url': f'http://host.docker.internal:{ddb_port}',
        'verify': False,
        'region_name': 'us-east-1',
        'aws_access_key_id': 'LOCAL_ACCESS_KEY_ID',
        'aws_secret_access_key': 'LOCAL_SECRET_ACCESS_KEY',
        'config': botocore.client.Config(connect_timeout=3, read_timeout=3)
    }


def get_table_operations(
        table_name: str,
        client: botocore.client.BaseClient = None,
        ) -> Tuple[Callable]:
    if client is None:
        client = boto3.client('dynamodb')

    put_manager = batch_put_manager

    def batch_get(
            keys: List[dict],
            client: 'boto3.client' = client,
            table_name: str = table_name,
            ) -> dict:
        return client.batch_get_item(
            RequestItems={
                table_name: {
                    'Keys': keys,
                },
            },
        )

    def batch_put(
            items: List[dict],
            max_queue_size: int = 0,  # 0 (zero) leads to infinite-sized queue
            client: 'boto3.client' = client,
            ddb_batch_put_size: int = BATCH_WRITE_MAX_SIZE,
            ) -> dict:
        responses = []

        batches = split_batch_items(items, ddb_batch_put_size)

        for batch_items in batches:
            responses.append(client.batch_write_item(
                RequestItems={
                    table_name: [
                        {'PutRequest': {'Item': item}}
                        for item in batch_items
                    ],
                },
            ))

        return responses

    def split_batch_items(items, batch_size):
        for i in range(0, len(items), batch_size):
            yield items[i:i + batch_size]

    return table_operations(batch_get, batch_put)


def batch_put_manager(
        items: List[dict],
        max_retries: int = BATCH_WRITE_MAX_RETRIES,
        batch_size: int = BATCH_WRITE_MAX_SIZE,
        max_queue_size: int = 0,  # 0 (zero) leads to infinite-sized queue
        ) -> NamedTuple:
    '''Handles batch limits, retry, exponential back-off for DynamoDB'''
    batch_queue = RetryAwareQueue(
        max_retries=max_retries,
        maxsize=max_queue_size,
    )
    queue_status = BatchQueueStatus.FULL
    responses = []

    def insert_items(
            items: List[dict],
            batch_queue: RetryAwareQueue = batch_queue,
            ) -> None:
        inserted_count = 0
        for item in items:
            try:
                batch_queue.put_aware(item)
                inserted_count += 1
            except queue.Full:
                not_inserted = len(items) - inserted_count
                log.error(f'## Queue is full! A Total of {not_inserted} '
                          'items were not inserted')

    def next_batch(
            batch_queue: RetryAwareQueue = batch_queue,
            queue_status: str = queue_status,
            batch_size: int = batch_size,
            ) -> list:
        items = []

        while len(items) < batch_size:
            try:
                item = batch_queue.get_aware()

                items.append(item)
            except queue.Empty:
                log.info(f'## DynamoDB batch queue "{str(batch_queue)}" is empty')  # NOQA
                batch_queue.status = BatchQueueStatus.EMPTY
                break

        return items

    def parse_response(
            response: dict,
            batch_queue: queue.Queue = batch_queue,
            queue_status: str = queue_status,
            batch_size: int = batch_size,
            ) -> None:
        pass

    insert_items(items=items)

    return manager(
        queue=batch_queue,
        status=queue_status,
        responses=responses,
        next_batch=next_batch,
        parse_response=parse_response,
    )
